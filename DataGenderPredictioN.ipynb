{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#importing Libraries required to run the model..\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "if int(os.environ.get(\"MODERN_PANDAS_EPUB\", 0)):\n",
    "    import prep # noqa\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.options.display.max_rows = 10\n",
    "sns.set(style='ticks', context='talk')\n",
    "# Read csv in a Spark\n",
    "#Gender = pd.read_csv(GenderFilePath,sep='^') \n",
    "#Gender.info()\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "pd.set_option('display.max_colwidth', -1)  # or 199\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from numpy import arange\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import set_option\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read csv in a Spark\n",
    "GenderFilePath = \"/usr/local/bin/Gender Prediction/Data/genderprediction.txt\"\n",
    "\n",
    "import pandas as pd\n",
    "Gender = pd.read_csv(GenderFilePath, delimiter='\\t', names=[\"UserId\", \"Gender\", \"Age\", \"Language\", \"description\", \"long_description\", \"URL\", \"DateTime\"])\n",
    "\n",
    "Gender.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Copy the dataframe\n",
    "df= Gender.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load Stop words\n",
    "    nltk.download('stopwords')\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Datapreprocess(df):\n",
    "    \n",
    "    # Filling long_description missing values with description column..\n",
    "    df['long_description'].fillna(df['description'])\n",
    "#droping missing values....\n",
    "    del df['description']\n",
    "    df.dropna(subset=['UserId'],inplace=True)\n",
    "    df.dropna(subset=['Gender'],inplace=True)\n",
    "    df.dropna(subset=['long_description'],inplace=True)\n",
    "    df.dropna(subset=['DateTime'],inplace=True)\n",
    "\n",
    "    df['Create Time'] = pd.to_datetime(df['DateTime'])\n",
    "\n",
    "    # Category Based Features \n",
    "    df['URL'].str.strip()\n",
    "    df['domain'] = df['URL'].str.split('//').str[-1].str.split('/').str[0]\n",
    "    df['Count'] = df['URL'].str.split('//').str[-1].str.split('/').str.len()\n",
    "    mask = df['Count'] > 2\n",
    "\n",
    "    df['Categ'] = np.where(mask,df['URL'].str.split('//').str[-1].str.split('/').str[1], 'No Category')\n",
    "    df['Subcateg'] = np.where(mask, df['URL'].str.split('//').str[-1].str.split('/').str[2], 'No Subcategory')\n",
    "\n",
    "    df['Categ'] = df.Categ.astype('category')\n",
    "    df['Subcateg']= df.Subcateg.astype('category')\n",
    "    df['domain'] = df.domain.astype('category')\n",
    "    df1=df[mask]\n",
    "    # Time n Squential Based\n",
    "    df1['Gender'] = df1['Gender'].astype('category')\n",
    "    df1[\"Create Time\"]= pd.to_datetime(df1['Create Time'])\n",
    "    df1['time_hour'] = df1['Create Time'].apply(lambda x: x.hour)\n",
    "    df1.Gender.replace(['M', 'F'], [0, 1], inplace=True)\n",
    "\n",
    "    df2 = df1[df1['long_description'].notna()]\n",
    "    def sent_to_words(sentences):\n",
    "        for sent in sentences:\n",
    "            sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
    "            sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "            sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "            sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
    "            yield(sent)  \n",
    "    # Convert to list\n",
    "    data_ld = df2.long_description.values.tolist()\n",
    "\n",
    "    data_words = list(sent_to_words(data_ld))\n",
    "     #Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "    def remove_stopwords(texts):\n",
    "        return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "    def make_bigrams(texts):\n",
    "        return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "    def make_trigrams(texts):\n",
    "        return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "    def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "        \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "        texts_out = []\n",
    "        for sent in texts:\n",
    "            doc = nlp(\" \".join(sent)) \n",
    "            texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "        return texts_out\n",
    "\n",
    "    # Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    # Remove Stop Words\n",
    "    data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "    # Form Bigrams\n",
    "    data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "    # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "    # python3 -m spacy download en\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "    # Create Corpus\n",
    "    texts = data_lemmatized\n",
    "\n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    #For Gibbs sampling using MAllet package...\n",
    "    mallet_path = '/usr/local/bin//mallet-2.0.8/bin/mallet' # update this path\n",
    "    ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)\n",
    "\n",
    "    # Build LDA model\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=20, \n",
    "                                               random_state=100,\n",
    "                                               update_every=1,\n",
    "                                               chunksize=100,\n",
    "                                               passes=10,\n",
    "                                               alpha='auto',\n",
    "                                               per_word_topics=True)\n",
    "\n",
    "    def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ld):\n",
    "        # Init output\n",
    "        sent_topics_df = pd.DataFrame()\n",
    "\n",
    "        # Get main topic in each document\n",
    "        for i, row in enumerate(ldamodel[corpus]):\n",
    "            row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "            # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "            for j, (topic_num, prop_topic) in enumerate(row):\n",
    "                if j == 0:  # => dominant topic\n",
    "                    wp = ldamodel.show_topic(topic_num)\n",
    "                    topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                    sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "                else:\n",
    "                    break\n",
    "        sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "        # Add original text to the end of the output\n",
    "        contents = pd.Series(texts)\n",
    "        sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "        return(sent_topics_df)\n",
    "\n",
    "    # Compute Coherence Score\n",
    "    def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "        \"\"\"\n",
    "        Compute c_v coherence for various number of topics\n",
    "    Parameters:\n",
    "        ----------\n",
    "        dictionary : Gensim dictionary\n",
    "        corpus : Gensim corpus\n",
    "        texts : List of input texts\n",
    "        limit : Max num of topics\n",
    "     Returns:\n",
    "        -------\n",
    "        model_list : List of LDA topic models\n",
    "        coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "        \"\"\"\n",
    "        coherence_values = []\n",
    "        model_list = []\n",
    "        for num_topics in range(start, limit, step):\n",
    "            model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "            model_list.append(model)\n",
    "            coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "            coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "        return model_list, coherence_values\n",
    "    coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "    model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=40, step=6)\n",
    "    # Select the model and print the topics\n",
    "    optimal_model = model_list[3]\n",
    "    model_topics = optimal_model.show_topics(formatted=False)\n",
    "\n",
    "    df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data_ld )\n",
    "\n",
    "    # Format\n",
    "    df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "    df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'long_description']\n",
    "\n",
    "    df3 = pd.merge(df1, df_dominant_topic, on='long_description')\n",
    "\n",
    "    params = [\"UserId\", \"Gender\", \"Create Time\",\"Categ\",\"time_hour\", \"URL\", \"Subcateg\", \"Topic_Perc_Contrib\", \"Age\", \"domain\"]\n",
    "\n",
    "    sqFeatures = df3[params]\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "    cat_columns = [ \"Gender\",\"Categ\",\"time_hour\", \"Subcateg\", \"Age\", \"domain\"]\n",
    "    sqFeatures[cat_columns].apply(lambda col: col.astype('category'))\n",
    "    # add it to our new dataframe\n",
    "    label_encoders = {}\n",
    "    for col in cat_columns:\n",
    "        print(\"Encoding {}\".format(col))\n",
    "        new_le = LabelEncoder()\n",
    "        sqFeatures[col] = pd.factorize(sqFeatures[col])[0]\n",
    "        sqFeatures[col] = new_le.fit_transform(sqFeatures[col])\n",
    "        label_encoders[col] = new_le  \n",
    "\n",
    "    sqFeatures = sqFeatures.groupby('UserId')[[ \"Gender\", \"Create Time\",\"Categ\",\"time_hour\", \"URL\", \"Subcateg\", \"Topic_Perc_Contrib\", \"Age\", \"domain\"]].agg(lambda x: x.tolist())\n",
    "\n",
    "\n",
    "    # Calculate Mutual Info..\n",
    "    from sklearn.metrics import mutual_info_score\n",
    "    def calc_MI(x, y, bins):\n",
    "        c_xy = np.histogram2d(x, y, bins)[0]\n",
    "        mi = mutual_info_score(None, None, contingency=c_xy)\n",
    "        return mi\n",
    "\n",
    "    sqFeatures['mutualInfo'] = sqFeatures.apply(lambda row: calc_MI(row['Gender'], row['Categ'], 30), axis=1)\n",
    "    sqFeatures['mutualInfo_sub'] = sqFeatures.apply(lambda row: calc_MI(row['Gender'], row['Subcateg'], 30), axis=1)\n",
    "    sqFeatures.loc[:, 'Gender'] = sqFeatures.Gender.map(lambda x: x[0])\n",
    "    sqFeatures.loc[:, 'Age'] = sqFeatures.Age.map(lambda x: x[0])\n",
    "    label1 = sqFeatures['Gender']\n",
    "\n",
    "    sqFeatures['domain'] = sqFeatures['domain'].apply(lambda x: np.array(x))\n",
    "    sqFeatures['Categ'] = sqFeatures['Categ'].apply(lambda x: np.array(x))\n",
    "    sqFeatures['time_hour'] = sqFeatures['time_hour'].apply(lambda x: np.array(x))\n",
    "    sqFeatures['Subcateg'] = sqFeatures['Subcateg'].apply(lambda x: np.array(x))\n",
    "    sqFeatures['Topic_Perc_Contrib'] = sqFeatures['Topic_Perc_Contrib'].apply(lambda x: np.array(x))\n",
    "\n",
    "    sqFeatures['mutualInfo'] = sqFeatures['mutualInfo'].apply(lambda x: np.array(x))\n",
    "    sqFeatures['mutualInfo_sub'] = sqFeatures['mutualInfo_sub'].apply(lambda x: np.array(x))\n",
    "\n",
    "    max_len = sqFeatures['domain'].apply(lambda x: len(x)).max()\n",
    "    print(max_len)\n",
    "\n",
    "    sqFeatures['Categ'] = sqFeatures['Categ'].apply(lambda x: np.pad(x, (0, max_len - len(x)), 'constant', constant_values=0))\n",
    "    sqFeatures['domain'] = sqFeatures['domain'].apply(lambda x: np.pad(x, (0, max_len - len(x)), 'constant', constant_values=0))\n",
    "    sqFeatures['mutualInfo_re'] =  pd.Series(np.pad(np.array(sqFeatures['mutualInfo']) .reshape(-1, 1), ((0, 0), (0, max_len-1)), 'constant').tolist(), index = sqFeatures.index)\n",
    "    sqFeatures['time_hour'] = sqFeatures['time_hour'].apply(lambda x: np.pad(x, (0, max_len - len(x)), 'constant', constant_values=0))        \n",
    "    sqFeatures['Subcateg'] = sqFeatures['Subcateg'].apply(lambda x: np.pad(x, (0, max_len - len(x)), 'constant', constant_values=0))\n",
    "    #sqFeatures['mutualInfo_sub'] = sqFeatures['mutualInfo_sub'].apply(lambda x: np.pad(x, (0, max_len - len(x)), 'constant', constant_values=0))\n",
    "    sqFeatures['Topic_Perc_Contrib'] = sqFeatures['Topic_Perc_Contrib'].apply(lambda x: np.pad(x, (0, max_len - len(x)), 'constant', constant_values=0))\n",
    "    Features = [\"Categ\",\"time_hour\", \"Subcateg\", \"Topic_Perc_Contrib\", \"domain\",\"mutualInfo_re\"]\n",
    "    sqFeatures['combined'] = sqFeatures[Features].values.tolist()\n",
    "    DF = sqFeatures[['Age','Gender','combined']]\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DF = Datapreprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "map(lambda x : x,DF['combined'].iloc[0:1])\n",
    "\n",
    "series = DF['combined'].apply(lambda x : np.array(x)).as_matrix().reshape(-1,1)\n",
    "\n",
    "features = np.apply_along_axis(lambda x : x[0], 1, series)\n",
    "\n",
    "y = DF['Gender'].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nsamples, nx, ny = features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = features.reshape(nsamples,nx*ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test options and evaluation metric\n",
    "num_folds = 10\n",
    "seed = np.random.seed\n",
    "scoring = 'accuracy'\n",
    "validation_size = 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(x, np.ravel(y), test_size = 0.2\n",
    "                                                    ,random_state = 0)\n",
    "print (X_train.shape, Y_train.shape)\n",
    "print (X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Spot-Check Algorithms\n",
    "models = []\n",
    "models.append(( ' LR ' , LogisticRegression()))\n",
    "models.append(( ' LDA ' , LinearDiscriminantAnalysis()))\n",
    "models.append(( ' KNN ' , KNeighborsClassifier()))\n",
    "models.append(( ' CART ' , DecisionTreeClassifier()))\n",
    "models.append(( ' NB ' , GaussianNB()))\n",
    "models.append(( ' SVM ' , SVC()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR',\n",
    "LogisticRegression())])))\n",
    "#pipelines.append(('ScaledLDA', Pipeline([('Scaler', StandardScaler()),('LDA',\n",
    "#LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN',\n",
    "KNeighborsClassifier())])))\n",
    "pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART',\n",
    "DecisionTreeClassifier())])))\n",
    "pipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()),('NB',\n",
    "GaussianNB())])))\n",
    "pipelines.append(('ScaledSVM', Pipeline([('Scaler', StandardScaler()),('SVM', SVC())])))\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "for name, model in pipelines:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Compare Algorithms\n",
    "fig = pyplot.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(( ' ScaledLR ' , Pipeline([( ' Scaler ' , StandardScaler()),( ' LR ' ,\n",
    "LogisticRegression())])))\n",
    "pipelines.append(( ' ScaledLDA ' , Pipeline([( ' Scaler ' , StandardScaler()),( ' LDA ' ,\n",
    "LinearDiscriminantAnalysis())])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipelines.append(( ' ScaledKNN ' , Pipeline([( ' Scaler ' , StandardScaler()),( ' KNN ' ,\n",
    "KNeighborsClassifier())])))\n",
    "pipelines.append(( ' ScaledCART ' , Pipeline([( ' Scaler ' , StandardScaler()),( ' CART ' ,\n",
    "DecisionTreeClassifier())])))\n",
    "pipelines.append(( ' ScaledNB ' , Pipeline([( ' Scaler ' , StandardScaler()),( ' NB ' ,\n",
    "GaussianNB())])))\n",
    "pipelines.append(( ' ScaledSVM ' , Pipeline([( ' Scaler ' , StandardScaler()),( ' SVM ' , SVC())])))\n",
    "results = []\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "fig = pyplot.figure()\n",
    "fig.suptitle( ' Scaled Algorithm Comparison ' )\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tune scaled KNN\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "neighbors = [1,3,5,7,9,11,13,15,17,19,21]\n",
    "param_grid = dict(n_neighbors=neighbors)\n",
    "model = KNeighborsClassifier()\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tune scaled SVM\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "c_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\n",
    "kernel_values = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "param_grid = dict(C=c_values, kernel=kernel_values)\n",
    "model = SVC()\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ensembles\n",
    "ensembles = []\n",
    "ensembles.append(('AB', AdaBoostClassifier()))\n",
    "ensembles.append(('GBM', GradientBoostingClassifier()))\n",
    "ensembles.append(('RF', RandomForestClassifier()))\n",
    "ensembles.append(('ET', ExtraTreesClassifier()))\n",
    "results = []\n",
    "names = []\n",
    "for name, model in ensembles:\n",
    "\tkfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "\tcv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "\tresults.append(cv_results)\n",
    "\tnames.append(name)\n",
    "\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "\tprint(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Compare Algorithms\n",
    "fig = pyplot.figure()\n",
    "fig.suptitle('Ensemble Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Finalize Model\n",
    "\n",
    "# prepare the model\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "model = SVC(C=1.5)\n",
    "model.fit(rescaledX, Y_train)\n",
    "# estimate accuracy on validation dataset\n",
    "rescaledValidationX = scaler.transform(X_test)\n",
    "predictions = model.predict(rescaledValidationX)\n",
    "print(accuracy_score(Y_test, predictions))\n",
    "print(confusion_matrix(Y_test, predictions))\n",
    "print(classification_report(Y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pandas import set_option\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y1  = DF['Age'].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Scoring = 'neg_mean_squared_error'\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(x, np.ravel(y1), test_size = 0.2\n",
    "                                                    ,random_state = 0)\n",
    "print (Xtrain.shape, Ytrain.shape)\n",
    "print (Xtest.shape, Ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Spot Check Algorithms\n",
    "algos = []\n",
    "algos.append(('LR', LinearRegression()))\n",
    "algos.append(('LASSO', Lasso()))\n",
    "algos.append(('EN', ElasticNet()))\n",
    "algos.append(('KNN', KNeighborsRegressor()))\n",
    "algos.append(('CART', DecisionTreeRegressor()))\n",
    "algos.append(('SVR', SVR()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate each model in turn\n",
    "out = []\n",
    "Names = []\n",
    "for Name, algo in algos:\n",
    "\tkfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "\tcv_results = cross_val_score(model, Xtrain, Ytrain, cv=kfold, scoring=Scoring)\n",
    "\tout.append(cv_results)\n",
    "\tNames.append(Name)\n",
    "\tMsg = \"%s: %f (%f)\" % (Name, cv_results.mean(), cv_results.std())\n",
    "\tprint(Msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "fig = pyplot.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(out)\n",
    "ax.set_xticklabels(Names)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Standardize the dataset\n",
    "Pipelines = []\n",
    "Pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR', LinearRegression())])))\n",
    "Pipelines.append(('ScaledLASSO', Pipeline([('Scaler', StandardScaler()),('LASSO', Lasso())])))\n",
    "Pipelines.append(('ScaledEN', Pipeline([('Scaler', StandardScaler()),('EN', ElasticNet())])))\n",
    "Pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsRegressor())])))\n",
    "Pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeRegressor())])))\n",
    "Pipelines.append(('ScaledSVR', Pipeline([('Scaler', StandardScaler()),('SVR', SVR())])))\n",
    "out = []\n",
    "Names = []\n",
    "for Name, algos in Pipelines:\n",
    "\tkfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "\tcv_results = cross_val_score(model, Xtrain, Ytrain, cv=kfold, scoring=Scoring)\n",
    "\tout.append(cv_results)\n",
    "\tNames.append(Name)\n",
    "\tmsg = \"%s: %f (%f)\" % (Name, cv_results.mean(), cv_results.std())\n",
    "\tprint(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "fig = pyplot.figure()\n",
    "fig.suptitle('Scaled Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(out)\n",
    "ax.set_xticklabels(Names)\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# KNN Algorithm tuning\n",
    "scaler = StandardScaler().fit(Xtrain)\n",
    "rescaleX = scaler.transform(Xtrain)\n",
    "k_values = np.array([1,3,5,7,9,11,13,15,17,19,21])\n",
    "param_grid = dict(n_neighbors=k_values)\n",
    "model = KNeighborsRegressor()\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "Grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=Scoring, cv=kfold)\n",
    "Grid_result = grid.fit(rescaleX, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ensembles\n",
    "ensembles = []\n",
    "ensembles.append(('ScaledAB', Pipeline([('Scaler', StandardScaler()),('AB', AdaBoostRegressor())])))\n",
    "ensembles.append(('ScaledGBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingRegressor())])))\n",
    "ensembles.append(('ScaledRF', Pipeline([('Scaler', StandardScaler()),('RF', RandomForestRegressor())])))\n",
    "ensembles.append(('ScaledET', Pipeline([('Scaler', StandardScaler()),('ET', ExtraTreesRegressor())])))\n",
    "out = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for Name, algos in ensembles:\n",
    "\tkfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "\tcv_results = cross_val_score(algos, Xtrain, Ytrain, cv=kfold, scoring=Scoring)\n",
    "\tout.append(cv_results)\n",
    "\tNames.append(Name)\n",
    "\tmsg = \"%s: %f (%f)\" % (Name, cv_results.mean(), cv_results.std())\n",
    "\tprint(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "fig = pyplot.figure()\n",
    "fig.suptitle('Scaled Ensemble Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(out)\n",
    "ax.set_xticklabels(Names)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tune scaled GBM\n",
    "scaler = StandardScaler().fit(Xtrain)\n",
    "rescaleX = scaler.transform(Xtrain)\n",
    "param_grid = dict(n_estimators=np.array([50,100,150,200,250,300,350,400]))\n",
    "model = GradientBoostingRegressor(random_state=seed)\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=Scoring, cv=kfold)\n",
    "grid_result = grid.fit(rescaleX, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make predictions on validation dataset\n",
    "\n",
    "# prepare the model\n",
    "scaler = StandardScaler().fit(Xtrain)\n",
    "rescaleX = scaler.transform(Xtrain)\n",
    "model = GradientBoostingRegressor(random_state=seed, n_estimators=400)\n",
    "model.fit(rescaleX, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# transform the validation dataset\n",
    "rescaledValidationX = scaler.transform(Xtest)\n",
    "predictions = model.predict(rescaledValidationX)\n",
    "print(mean_squared_error(Ytest, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "1. https://machinelearningmastery.com/\n",
    "2. https://www.researchgate.net/publication/278399485_Gender_Prediction_Using_Browsing_History"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
